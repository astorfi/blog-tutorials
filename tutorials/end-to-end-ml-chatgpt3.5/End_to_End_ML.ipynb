{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6rgkrFN1S9N"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install openai pandas numpy sklearn awscli boto3 sagemaker\n",
        "\n",
        "# Import the required libraries\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import boto3\n",
        "import sagemaker\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'your-api-key'\n"
      ],
      "metadata": {
        "id": "HqgtmVVb10Bo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data using GPT-3.5\n",
        "response = openai.Completion.create(\n",
        "  engine=\"gpt-3.5-turbo\",\n",
        "  prompt=\"Generate a dataset of 1000 examples for binary classification with two features, 'age' and 'income'. The label should be 'can afford luxury car'.\",\n",
        "  max_tokens=5000\n",
        ")\n",
        "\n",
        "# Extract the text\n",
        "data_text = response.choices[0].text.strip()\n",
        "\n",
        "# Transform the data into a structured format\n",
        "data_lines = data_text.split(\"\\n\")\n",
        "data_dict = {'age': [], 'income': [], 'can_afford_luxury_car': []}\n",
        "\n",
        "for line in data_lines:\n",
        "    age, income, can_afford = line.split(\",\")\n",
        "    data_dict['age'].append(float(age))\n",
        "    data_dict['income'].append(float(income))\n",
        "    data_dict['can_afford_luxury_car'].append(int(can_afford))\n",
        "\n",
        "# Load into a DataFrame\n",
        "df = pd.DataFrame(data_dict)\n"
      ],
      "metadata": {
        "id": "e0-jIOC214nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test set\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Train the model with GPT-3.5\n",
        "training_prompt = f\"Train a binary classification model with the following training data:\\n{train_df.to_csv(index=False)}\"\n",
        "response = openai.Completion.create(\n",
        "  engine=\"gpt-3.5-turbo\",\n",
        "  prompt=training_prompt,\n",
        "  max_tokens=500\n",
        ")\n",
        "\n",
        "# Extract the model\n",
        "model_text = response.choices[0].text.strip()\n"
      ],
      "metadata": {
        "id": "t57x-aNv17GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to CSV and save\n",
        "train_df.to_csv('train.csv', index=False, header=False)\n",
        "test_df.to_csv('test.csv', index=False, header=False)\n",
        "\n",
        "# Upload the dataset to S3\n",
        "sagemaker_session = sagemaker.Session()\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = 'gpt-3.5-demo'\n",
        "\n",
        "train_location = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)\n",
        "test_location = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)\n",
        "\n",
        "# Define the SageMaker estimator\n",
        "from sagemaker import get_execution_role\n",
        "role = get_execution_role()\n",
        "container = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, 'latest')\n",
        "\n",
        "xgb = sagemaker.estimator.Estimator(container,\n",
        "                                    role, \n",
        "                                    instance_count=1, \n",
        "                                    instance_type='ml.m4.xlarge',\n",
        "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
        "                                    sagemaker_session=sagemaker_session)\n",
        "\n",
        "# Set hyperparameters and fit the model\n",
        "xgb.set_hyperparameters(max_depth=5,\n",
        "                        eta=0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        objective='binary:logistic',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=200)\n",
        "\n",
        "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
        "s3_input_test = sagemaker.inputs.TrainingInput(s3_data=test_location, content_type='csv')\n",
        "\n",
        "xgb.fit({'train': s3_input_train, 'validation': s3_input_test})\n",
        "\n",
        "# Deploy the model\n",
        "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n"
      ],
      "metadata": {
        "id": "CFTqDYvL19ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}