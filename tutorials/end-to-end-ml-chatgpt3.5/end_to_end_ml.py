# -*- coding: utf-8 -*-
"""End-to-End-ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PzUaQbU3_j5pzq1Xy6lFkKGU5PasRSkf
"""

# Install the necessary libraries
!pip install openai pandas numpy sklearn awscli boto3 sagemaker

# Import the required libraries
import openai
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import boto3
import sagemaker

openai.api_key = 'your-api-key'

# Generate synthetic data using GPT-3.5
response = openai.Completion.create(
  engine="gpt-3.5-turbo",
  prompt="Generate a dataset of 1000 examples for binary classification with two features, 'age' and 'income'. The label should be 'can afford luxury car'.",
  max_tokens=5000
)

# Extract the text
data_text = response.choices[0].text.strip()

# Transform the data into a structured format
data_lines = data_text.split("\n")
data_dict = {'age': [], 'income': [], 'can_afford_luxury_car': []}

for line in data_lines:
    age, income, can_afford = line.split(",")
    data_dict['age'].append(float(age))
    data_dict['income'].append(float(income))
    data_dict['can_afford_luxury_car'].append(int(can_afford))

# Load into a DataFrame
df = pd.DataFrame(data_dict)

# Split data into training and test set
train_df, test_df = train_test_split(df, test_size=0.2)

# Train the model with GPT-3.5
training_prompt = f"Train a binary classification model with the following training data:\n{train_df.to_csv(index=False)}"
response = openai.Completion.create(
  engine="gpt-3.5-turbo",
  prompt=training_prompt,
  max_tokens=500
)

# Extract the model
model_text = response.choices[0].text.strip()

# Convert DataFrame to CSV and save
train_df.to_csv('train.csv', index=False, header=False)
test_df.to_csv('test.csv', index=False, header=False)

# Upload the dataset to S3
sagemaker_session = sagemaker.Session()
bucket = sagemaker_session.default_bucket()
prefix = 'gpt-3.5-demo'

train_location = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=prefix)
test_location = sagemaker_session.upload_data('test.csv', bucket=bucket, key_prefix=prefix)

# Define the SageMaker estimator
from sagemaker import get_execution_role
role = get_execution_role()
container = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, 'latest')

xgb = sagemaker.estimator.Estimator(container,
                                    role, 
                                    instance_count=1, 
                                    instance_type='ml.m4.xlarge',
                                    output_path='s3://{}/{}/output'.format(bucket, prefix),
                                    sagemaker_session=sagemaker_session)

# Set hyperparameters and fit the model
xgb.set_hyperparameters(max_depth=5,
                        eta=0.2,
                        gamma=4,
                        min_child_weight=6,
                        subsample=0.8,
                        objective='binary:logistic',
                        early_stopping_rounds=10,
                        num_round=200)

s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')
s3_input_test = sagemaker.inputs.TrainingInput(s3_data=test_location, content_type='csv')

xgb.fit({'train': s3_input_train, 'validation': s3_input_test})

# Deploy the model
xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')